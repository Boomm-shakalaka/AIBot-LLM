import asyncio
import sys
import streamlit as st
from config_setting import prompt_config
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from dotenv import find_dotenv, load_dotenv
from config_setting import model_config
from langchain_community.chat_models import QianfanChatEndpoint
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import crawler_modules

class SearchBot:
    def __init__(self):
        """
        åˆå§‹åŒ–SearchBotç±»çš„å®ä¾‹ï¼ŒåŠ è½½ç¯å¢ƒå˜é‡å¹¶è®¾ç½®æ¨¡å‹é€‰é¡¹å’Œä»¤ç‰Œæ•°ã€‚
        """
        load_dotenv(find_dotenv())#åŠ è½½ç¯å¢ƒå˜é‡.env
        self.llm = None
        self.content=None
    
    def init_llm_model(self,select_platform,select_model,select_temperature):
        """
        åˆå§‹åŒ–æ¨¡å‹ã€‚

        Parameters:
        select_platform (str): é€‰æ‹©çš„æ¨¡å‹å¹³å°ã€‚
        select_model (str): é€‰æ‹©çš„æ¨¡å‹ã€‚
        """
        model_tokens = model_config.model_description_ls[select_model]["tokens"]
        if select_platform=='ç™¾åº¦äº‘å¹³å°':
            self.llm = QianfanChatEndpoint(model=select_model,temperature=select_temperature)
        elif select_platform=='Groqå¹³å°':
            self.llm = ChatGroq(model_name=select_model,max_tokens=model_tokens,temperature=select_temperature)
        elif select_platform=='Siliconflowå¹³å°':
            self.llm = ChatOpenAI(model_name=select_model,base_url="https://api.siliconflow.cn/v1",temperature=select_temperature)

    def generate_based_history_query(self,question,chat_history):
        """
        æ ¹æ®é—®é¢˜å’Œå¯¹è¯å†å²ç”Ÿæˆæœç´¢æŸ¥è¯¢ã€‚

        Parameters:
        question (str): ç”¨æˆ·çš„é—®é¢˜ã€‚
        chat_history (list): å¯¹è¯å†å²åˆ—è¡¨ã€‚

        Returns:
        str: ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢ã€‚
        """ 
        prompt=PromptTemplate.from_template(prompt_config.query_generated_prompt)
        rag_chain = prompt | self.llm | StrOutputParser()
        result=rag_chain.invoke(
            {
                "chat_history":chat_history, 
                "question": question
            }
        )
        return result
    
    def judge_search(self,question,chat_history):
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œæœç´¢ã€‚

        Parameters:
        question (str): ç”¨æˆ·çš„é—®é¢˜ã€‚
        chat_history (list): å¯¹è¯å†å²åˆ—è¡¨ã€‚

        Returns:
        str: åˆ¤æ–­ç»“æœã€‚
        """
        judge_model=QianfanChatEndpoint(model='ERNIE-Lite-8K')
        prompt = PromptTemplate.from_template(prompt_config.judge_search_prompt)
        chain = prompt | judge_model | StrOutputParser()
        response = chain.invoke({
            "chat_history": chat_history,
            "question": question,
        })
        return response
    
    def get_response(self, question,select_search_type,chat_history):
        """
        æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œå¯¹è¯å†å²è·å–å“åº”ã€‚

        Parameters:
        question (str): ç”¨æˆ·çš„é—®é¢˜ã€‚
        select_search_type (str): é€‰æ‹©çš„æœç´¢ç±»å‹ã€‚
        chat_history (list): å¯¹è¯å†å²åˆ—è¡¨ã€‚

        Returns:
        stræˆ–generator: å¦‚æœä½¿ç”¨æµå¼è¾“å‡ºï¼Œè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼›å¦åˆ™è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
        """   
        try:
            judge_result=self.judge_search(question,chat_history)#åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢
            if 'yes' in judge_result:#éœ€è¦æœç´¢
                query=self.generate_based_history_query(question,chat_history)#ç”Ÿæˆæœç´¢query
                if select_search_type=="duckduckgo":
                    self.content=crawler_modules.duckduck_search(query)
                else:
                    sys_type=sys.platform
                    if sys_type == "win32":
                        loop = asyncio.ProactorEventLoop()#windowsç³»ç»Ÿ
                    else:
                        loop = asyncio.SelectorEventLoop()#linuxç³»ç»Ÿ
                    self.content = loop.run_until_complete(crawler_modules.google_search_async(query))#å¼‚æ­¥æœç´¢
                prompt = ChatPromptTemplate.from_template(prompt_config.searchbot_prompt)
                chain = prompt | self.llm | StrOutputParser()
                return chain.stream({
                    "chat_history": chat_history,
                    "question": question,
                    "content": self.content
                })
            else:
                prompt = ChatPromptTemplate.from_template(prompt_config.chatbot_prompt)#ä¸éœ€è¦æœç´¢ï¼Œç›´æ¥chat
                chain = prompt | self.llm | StrOutputParser()
                return chain.stream({
                    "chat_history": chat_history,
                    "question": question,
                })
        except Exception as e:
            return f"å½“å‰æ¨¡å‹{self.model_option}æš‚ä¸å¯ç”¨ï¼Œè¯·åœ¨å·¦ä¾§æ é€‰æ‹©å…¶ä»–æ¨¡å‹ã€‚"

def init_params():
    """
    åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å‚æ•°ã€‚

    å¦‚æœä¼šè¯çŠ¶æ€ä¸­ä¸å­˜åœ¨"search_message"é”®ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨å¹¶å°†å…¶èµ‹å€¼ç»™"search_message"ã€‚
    å¦‚æœä¼šè¯çŠ¶æ€ä¸­ä¸å­˜åœ¨"searchbot"é”®ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„SearchBotå®ä¾‹å¹¶å°†å…¶èµ‹å€¼ç»™"searchbot"ã€‚
    """
    if "search_message" not in st.session_state:
        st.session_state.search_message=[]
    if "searchbot" not in st.session_state:
        st.session_state.search_bot = SearchBot()

def clear():
    """
    æ¸…é™¤ä¼šè¯çŠ¶æ€ä¸­çš„æœç´¢è®°å½•å’ŒSearchBotå®ä¾‹ã€‚

    å°†ä¼šè¯çŠ¶æ€ä¸­çš„"search_message"é”®å¯¹åº”çš„å€¼é‡ç½®ä¸ºç©ºåˆ—è¡¨ã€‚
    åˆ›å»ºä¸€ä¸ªæ–°çš„SearchBotå®ä¾‹å¹¶å°†å…¶èµ‹å€¼ç»™"searchbot"é”®ã€‚
    """
    st.session_state.search_message = []
    st.session_state.search_bot = SearchBot()

def online_chat_page():
    init_params() # åˆå§‹åŒ–æ¨¡å‹å’ŒèŠå¤©è®°å½•
    '''é¡µé¢å¸ƒå±€'''  
    with st.sidebar:
        with st.container(border=True):
            select_platform=st.selectbox("é€‰æ‹©æ¨¡å‹å¹³å°",options=list(model_config.model_platform_ls.keys()))#æ¨¡å‹é€‰æ‹©
            select_model=st.selectbox("é€‰æ‹©æ¨¡å‹",options=model_config.model_platform_ls[select_platform]) 
            select_temperature=st.slider("æ¸©åº¦ç³»æ•°",min_value=0.1,max_value=1.0,step=0.1,value=0.7,help='æ•°å€¼ä½è¾“å‡ºæ›´å…·ç¡®å®šå’Œä¸€è‡´æ€§ï¼Œæ•°å€¼é«˜æ›´å…·åˆ›é€ å’Œå¤šæ ·æ€§')#æ¸©åº¦é€‰æ‹©
            select_search_type=st.selectbox("é€‰æ‹©æœç´¢å¼•æ“æ¨¡å‹",options=["duckduckgo","åŸºäºè‡ªåŠ¨åŒ–çˆ¬è™«æœç´¢"],index=1)
            st.button(label="æ¸…é™¤èŠå¤©è®°å½•", on_click=lambda: clear(),use_container_width=True)
    st.title("ğŸŒåœ¨çº¿èŠå¤©æœºå™¨äºº")
    st.subheader(body='',divider="rainbow")
    
    '''æ»šåŠ¨æ›´æ–°èŠå¤©è®°å½•'''
    with st.chat_message("AI"):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯åŸºäºåœ¨çº¿å¼•æ“çš„AIèŠå¤©æœºå™¨äººï¼Œå¿…è¦æ—¶æˆ‘ä¼šæ ¹æ®å®æ—¶ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚æ­¤å¤–åœ¨æˆ‘çš„å·¦ä¾§æ ä¸­ï¼Œæ‚¨å¯ä»¥æ›´æ¢ä¸åŒçš„AIæ¨¡å‹å’Œçˆ¬è™«æ¨¡å‹ã€‚")
    for message in st.session_state.search_message:
        if isinstance(message, AIMessage):
            with st.chat_message("AI"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("Human"):
                st.markdown(message.content)

    '''ç”¨æˆ·é—®é¢˜äº¤äº’'''
    question = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜")
    if question:
        with st.chat_message("Human"):
            st.markdown(question)
            st.session_state.search_message.append(HumanMessage(content=question))
        with st.chat_message("AI"):
            with st.spinner('æ€è€ƒä¸­....'):
                st.session_state.search_bot.init_llm_model(select_platform,select_model,select_temperature)
                response =  st.write_stream(st.session_state.search_bot.get_response(question,select_search_type,st.session_state.search_message))#æµå¼è¾“å‡º
            st.session_state.search_message.append(AIMessage(content=response))


