import asyncio
import os
import streamlit as st
from config_setting import prompt_config,model_config
import requests
from langchain_core.messages import AIMessage, HumanMessage
from dotenv import find_dotenv, load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_groq import ChatGroq
from langchain_cohere import CohereEmbeddings
import crawler_modules
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from config_setting import model_config
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.chat_models import QianfanChatEndpoint

class urlbot:
    def __init__(self):
        load_dotenv(find_dotenv())
        self.model_option = None
        self.model_tokens = None
        self.llm = None
        self.context = None

    def generate_based_history_query(self,question,chat_history):
        rag_chain = PromptTemplate.from_template(prompt_config.query_generated_prompt) | self.llm | StrOutputParser()
        result=rag_chain.invoke(
            {
                "chat_history":chat_history , 
                "question": question
            }
        )
        return result

    def get_response(self,question,chat_history,vectorstore):
        try:
            '''get context'''
            if self.model_option =='ERNIE-Lite-8K' or self.model_option=='ERNIE-speed-128k':
                self.llm = QianfanChatEndpoint(model=self.model_option)
            else:
                self.llm = ChatGroq(model_name=self.model_option,temperature=0.1,max_tokens=self.model_tokens)
            query=self.generate_based_history_query(question,chat_history)
            self.context = vectorstore.max_marginal_relevance_search(query=query, k=8)
            '''é™åˆ¶contexté•¿åº¦'''
            context_len,chat_history_len=0,0
            if self.context is not None:
                context_len=len(self.context)
            if chat_history is not None:
                chat_history_len=len(chat_history)
            if context_len+chat_history_len-200 >self.model_tokens:
                self.context=self.context[:self.model_tokens-200]#é™åˆ¶contexté•¿åº¦
                chat_history=chat_history[:self.model_tokens-200]#é™åˆ¶chat_historyé•¿åº¦

            '''get response'''
            chain = PromptTemplate.from_template(prompt_config.qa_retrieve_prompt) | self.llm | StrOutputParser()

            return chain.stream({
                    "chat_history":chat_history, 
                    "question": question,
                    "context": self.context
                })
            # result=chain.invoke({"chat_history":chat_history, "question": question,"context": self.context})
            # return result
        except Exception as e:
            return f"å½“å‰æ£€ç´¢æš‚ä¸å¯ç”¨ï¼Œè¯·åœ¨å·¦ä¾§æ æ›´æ¢æ¨¡å‹ï¼Œæˆ–è€…é€‰æ‹©å…¶ä»–åŠŸèƒ½ã€‚"

        
def init_params():
    if "url_messages" not in st.session_state:
        st.session_state.url_messages = []
    if "url" not in st.session_state:
        st.session_state.url=''
    if "url_bot" not in st.session_state:
        st.session_state.url_bot = urlbot()
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = None

def request_website(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36',
    }
    try:
        response = requests.get(url,headers=headers)
        response.raise_for_status()  # If the response status code is not 200, an exception is thrown
        return response.text
    except requests.RequestException as e:
        return None
    

def clear():
    if "url_messages" in st.session_state:
        st.session_state.url_messages = []
    if "url" in st.session_state:
        st.session_state.url=''
    if "url_bot" in st.session_state:
        st.session_state.url_bot = urlbot()
    if "vectorstore" in st.session_state:
        st.session_state.vectorstore = None


def url_parser(url, select_crawler):
    parse_flag = False
    docs = ''
    try:
        if select_crawler == "Selenium":
            docs = crawler_modules.selenium_url_crawler(url)
        else:
            loop = asyncio.ProactorEventLoop()
            docs = loop.run_until_complete(crawler_modules.playwright_crawler_async(url))
        if len(docs[0].page_content) < 100:#åˆ¤æ–­æ˜¯å¦è§£ææˆåŠŸ
            return docs, parse_flag
        parse_flag = True
        return docs, parse_flag
    except Exception as e:
        return docs, parse_flag

def retrieve_data(docs):
    try:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap = 200)
        splits = text_splitter.split_documents(docs)
        embeddings = CohereEmbeddings(model="embed-multilingual-v3.0")
        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
        return vectorstore
    except Exception as e:
        return f"å½“å‰æ–‡æœ¬åµŒå…¥å‘é‡å¤„ç†äº§ç”Ÿé—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ï¼Œæˆ–è€…é€‰æ‹©å…¶ä»–AIåŠŸèƒ½"
def url_page():
    init_params()
    '''é¡µé¢å¸ƒå±€'''
    with st.sidebar:
        with st.container(border=True):
            select_model=st.selectbox("é€‰æ‹©æ¨¡å‹",options=["ç™¾åº¦åƒå¸†å¤§æ¨¡å‹-128k","ç™¾åº¦åƒå¸†å¤§æ¨¡å‹-8k","è°·æ­ŒGemmaå¤§æ¨¡å‹","è°·æ­Œgeminiå¤§æ¨¡å‹","Llama3-70bå¤§æ¨¡å‹","Llama3-8bå¤§æ¨¡å‹","Mixtralå¤§æ¨¡å‹"],index=0)
            model_option=model_config.model_ls[select_model]["name"]
            model_tokes=model_config.model_ls[select_model]["tokens"]
            select_crawler=st.selectbox("é€‰æ‹©çˆ¬è™«",options=["Playwright","Selenium"],index=0)
            show_retrive=st.checkbox("å±•ç¤ºæ£€ç´¢æ¥æº",value=False)
            st.button(label="æ¸…é™¤èŠå¤©è®°å½•", on_click=lambda: clear(),use_container_width=True)
    st.title("ğŸ”—URLè§£æ-AIæœºå™¨äºº")
    st.subheader(body='',divider="rainbow")

    '''URLè¾“å…¥æ¡†'''
    with st.container(border=True):
        st.session_state.url=st.text_input(label='è¯·è¾“å…¥æ‚¨æƒ³è¦æœç´¢çš„URL:',value='https://lilianweng.github.io/posts/2023-06-23-agent/')
        confirm_btn=st.button("ç¡®è®¤")
    
    '''æ»šåŠ¨æ›´æ–°èŠå¤©è®°å½•'''
    with st.chat_message("AI"):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯åŸºäºAIå¤§æ¨¡å‹çš„ç½‘é¡µè§£ææœºå™¨äººã€‚æ‚¨å¯ä»¥ç°åœ¨åœ¨ä¸Šæ–¹è¾“å…¥æ¡†è¾“å…¥æ‚¨è¦æ£€ç´¢çš„ç½‘ç«™(https/http)")
    for message in st.session_state.url_messages:
        if isinstance(message, AIMessage):
            with st.chat_message("AI"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("Human"):
                st.markdown(message.content)

    '''URLè§£æ'''
    if confirm_btn:#ç¡®è®¤URL
        st.session_state.url_messages = [] #æ¸…é™¤èŠå¤©è®°å½•
        url_response = request_website(st.session_state.url) #è¯·æ±‚ç½‘é¡µ
        if url_response is not None: #åˆ¤æ–­æ˜¯å¦è¯·æ±‚æˆåŠŸ
            parse_flag = True #è§£ææˆåŠŸ
            with st.chat_message("AI"):
                with st.status("Load Website, please wait..", expanded=True) as status:
                    st.markdown("Parsing data on URL...")
                    docs, parse_flag=url_parser(st.session_state.url,select_crawler) #è§£æç½‘é¡µ
                    if parse_flag==False: #è§£æå¤±è´¥
                        st.markdown("Parsing failed, please change the crawler model or URL")
                        status.update(label="è§£æå¤±è´¥ï¼Œè¯·æ›´æ¢çˆ¬è™«æ¨¡å‹æˆ–è€…URL", state="error", expanded=False)
                    else:#è§£ææˆåŠŸï¼Œå¼€å§‹å‘é‡åµŒå…¥
                        st.markdown("Embedding the data...")
                        st.session_state.vectorstore=retrieve_data(docs)
                        status.update(label="Parsing complete!", state="complete", expanded=False)
                if parse_flag:  #è§£ææˆåŠŸ            
                    tmp_text = f"\n\nå½“å‰è®¿é—®URLä¸º: {st.session_state.url}.\n\næ‚¨å¯ä»¥è¯¢é—®å…³äºè¯¥ç½‘é¡µçš„é—®é¢˜ã€‚å¦‚æœæ‚¨æƒ³æŸ¥è¯¢åˆ«çš„ç½‘é¡µ,è¯·ç›´æ¥è¾“å…¥æ–°çš„URL"
                    st.markdown(tmp_text) #å±•ç¤ºè§£ææˆåŠŸä¿¡æ¯     
        else:#è¯·æ±‚å¤±è´¥
            tmp_text = f'URL: ( {st.session_state.url} ) æ— æ³•è®¿é—®,è¯·æ£€æŸ¥ç½‘é¡µæ˜¯å¦æ­£ç¡®æˆ–è€…è¯¥ç½‘é¡µæ— æ³•è¿æ¥(éœ€è¦åŒ…å«httpæˆ–è€…https)'
            with st.chat_message("AI"):
                st.markdown(tmp_text)
    
    '''ç”¨æˆ·é—®é¢˜äº¤äº’'''
    question = st.chat_input()
    if question and st.session_state.vectorstore:#åˆ¤æ–­æ˜¯å¦æœ‰é—®é¢˜,å¹¶ä¸”å·²ç»è§£ææˆåŠŸ
        with st.chat_message("Human"):
            st.markdown(question)
            st.session_state.url_messages.append(HumanMessage(content=question))
        with st.chat_message("AI"):
            with st.spinner('æ£€ç´¢ä¸­....'):
                st.session_state.url_bot.model_option = model_option
                st.session_state.url_bot.model_tokens = model_tokes
                chat_history = st.session_state.url_messages
                vectorstore = st.session_state.vectorstore
                # response=st.session_state.url_bot.get_response(question,chat_history,vectorstore)
                response = st.write_stream(st.session_state.url_bot.get_response(question,chat_history,vectorstore))#æµå¼è¾“å‡º
                st.session_state.url_messages.append(AIMessage(content=response))
            if show_retrive:#å±•ç¤ºæ£€ç´¢æ¥æº
                with st.expander("æ¥æºå†…å®¹"):
                    for i in range(len(st.session_state.url_bot.context)):
                        st.markdown(f"ç¬¬{i}ä¸ªç›¸å…³å†…å®¹: {st.session_state.url_bot.context[i].page_content}")


