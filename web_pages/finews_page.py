import random
from langchain_cohere import CohereEmbeddings
import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langchain_community.chat_models import QianfanChatEndpoint
from config_setting import model_config,prompt_config
from dotenv import find_dotenv, load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
import os
import re
class finewsbot():
    def __init__(self):
        """
        åˆå§‹åŒ–ChatBotç±»çš„å®ä¾‹ï¼ŒåŠ è½½ç¯å¢ƒå˜é‡å¹¶è®¾ç½®æ¨¡å‹é€‰é¡¹å’Œä»¤ç‰Œæ•°ã€‚
        """
        load_dotenv(find_dotenv())#åŠ è½½ç¯å¢ƒå˜é‡
        self.llm = None
        self.model_tokens=4096
        self.context = None
    
    def init_llm_model(self,select_platform,select_model,select_temperature):
        """
        åˆå§‹åŒ–æ¨¡å‹ã€‚
        """
        self.model_tokens = model_config.model_description_ls[select_model]["tokens"]
        if select_platform=='ç™¾åº¦äº‘å¹³å°':
            self.llm = QianfanChatEndpoint(model=select_model,temperature=select_temperature)
        elif select_platform=='Groqå¹³å°':
            self.llm = ChatGroq(model_name=select_model,max_tokens=self.model_tokens,temperature=select_temperature)
        elif select_platform=='Siliconflowå¹³å°':
            self.llm = ChatOpenAI(model_name=select_model,base_url="https://api.siliconflow.cn/v1",temperature=select_temperature)
    
    def generate_based_history_query(self,question,chat_history):
        rag_chain = PromptTemplate.from_template(prompt_config.query_generated_prompt) | self.llm | StrOutputParser()
        result=rag_chain.invoke(
            {
                "chat_history":chat_history , 
                "question": question
            }
        )
        return result
    
    def get_response(self,question,chat_history,vectorstore):
        '''get context'''
        query=self.generate_based_history_query(question,chat_history)
        self.context = vectorstore.max_marginal_relevance_search(query=query, k=10)
        '''é™åˆ¶contexté•¿åº¦'''
        context_len,chat_history_len=0,0
        if self.context is not None:
            context_len=len(self.context)
        if chat_history is not None:
            chat_history_len=len(chat_history)
        if context_len+chat_history_len-200 >self.model_tokens:
            self.context=self.context[:self.model_tokens-200]#é™åˆ¶contexté•¿åº¦
            chat_history=chat_history[:self.model_tokens-200]#é™åˆ¶chat_historyé•¿åº¦
        '''get response'''
        chain = PromptTemplate.from_template(prompt_config.finews_retrieve_prompt) | self.llm | StrOutputParser()
        return chain.stream({
                "chat_history":chat_history, 
                "question": question,
                "context": self.context
            })
            # result=chain.invoke({"chat_history":chat_history, "question": question,"context": self.context})
            # return result
    # except Exception as e:
    #     return f"å½“å‰æ£€ç´¢æš‚ä¸å¯ç”¨ï¼Œè¯·åœ¨å·¦ä¾§æ æ›´æ¢æ¨¡å‹ï¼Œæˆ–è€…é€‰æ‹©å…¶ä»–åŠŸèƒ½ã€‚"

def get_latest_db_file():
    """
    è·å–æœ€æ–°çš„æ•°æ®åº“æ–‡ä»¶ã€‚
    è·å–å½“å‰è„šæœ¬çš„ç›®å½•ï¼Œç„¶åè·å–é¡¹ç›®æ ¹ç›®å½•ã€‚
    è·å–æ‰€æœ‰ä»¥"chroma_db_"å¼€å¤´çš„æ–‡ä»¶åã€‚
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥æå–æ—¶é—´æˆ³ã€‚
    ä½¿ç”¨maxå‡½æ•°æ‰¾å‡ºæœ€æ–°çš„æ–‡ä»¶ã€‚
    è¿”å›æœ€æ–°çš„æ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›Noneã€‚
    """
    # è·å–å½“å‰è„šæœ¬çš„ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆE:\Desktop\AIBot-LLMï¼‰
    root_dir = os.path.dirname(current_dir)
    # è·å–æ‰€æœ‰ä»¥"chroma_db_"å¼€å¤´çš„æ–‡ä»¶å
    chroma_db_files = [f for f in os.listdir(root_dir) if f.startswith("chroma_db_")]
    # å®šä¹‰ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ¥æå–æ—¶é—´æˆ³
    pattern = re.compile(r'chroma_db_(\d{12})')
    # ä½¿ç”¨maxå‡½æ•°æ‰¾å‡ºæœ€æ–°çš„æ–‡ä»¶
    latest_db_name = None
    latest_timestamp = None
    for file in chroma_db_files:
        match = pattern.match(file)
        if match:
            timestamp = match.group(1)
            if latest_timestamp is None or timestamp > latest_timestamp:
                latest_timestamp = timestamp
                latest_db_name = file
    
    return root_dir,latest_db_name
    # except Exception as e:
    #     return None

def init_params():
    """
    åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ä¸­çš„èŠå¤©è®°å½•å’Œæ¨¡å‹å®ä¾‹ã€‚
    å°†ä¼šè¯çŠ¶æ€ä¸­çš„"finews_message"é”®å¯¹åº”çš„å€¼é‡ç½®ä¸ºç©ºåˆ—è¡¨ã€‚
    åˆ›å»ºä¸€ä¸ªæ–°çš„finewsbotå®ä¾‹å¹¶å°†å…¶èµ‹å€¼ç»™"finews_bot"é”®ã€‚
    """

    if "finews_message" not in st.session_state:
        st.session_state.finews_message = []
    if "finews_bot" not in st.session_state:
        st.session_state.finews_bot = finewsbot()
    if "finews_vectorstore" not in st.session_state:
        st.session_state.finews_vectorstore = None

def clear():
    """
    æ¸…é™¤ä¼šè¯çŠ¶æ€ä¸­çš„èŠå¤©è®°å½•å’Œæ¨¡å‹å®ä¾‹ã€‚

    å°†ä¼šè¯çŠ¶æ€ä¸­çš„"finews_message"é”®å¯¹åº”çš„å€¼é‡ç½®ä¸ºç©ºåˆ—è¡¨ã€‚
    åˆ›å»ºä¸€ä¸ªæ–°çš„finewsbotå®ä¾‹å¹¶å°†å…¶èµ‹å€¼ç»™"finews_bot"é”®ã€‚

    """
    st.session_state.finews_message = [] #æ¸…é™¤èŠå¤©è®°å½•
    st.session_state.finews_bot = finewsbot() #é‡æ–°åˆå§‹åŒ–æ¨¡å‹
    if "finews_vectorstore" not in st.session_state:
        st.session_state.finews_vectorstore = None

def finews_page():
    init_params()#åˆå§‹åŒ–æ¨¡å‹å’ŒèŠå¤©è®°å½•
    '''é¡µé¢å¸ƒå±€'''    
    with st.sidebar:
        with st.container(border=True):
            select_platform=st.selectbox("é€‰æ‹©æ¨¡å‹å¹³å°",options=list(model_config.model_platform_ls.keys()))#æ¨¡å‹é€‰æ‹©
            select_model=st.selectbox("é€‰æ‹©æ¨¡å‹",options=model_config.model_platform_ls[select_platform]) 
            select_temperature=st.slider("æ¸©åº¦ç³»æ•°",min_value=0.1,max_value=1.0,step=0.1,value=0.7,help='æ•°å€¼ä½è¾“å‡ºæ›´å…·ç¡®å®šå’Œä¸€è‡´æ€§ï¼Œæ•°å€¼é«˜æ›´å…·åˆ›é€ å’Œå¤šæ ·æ€§')#æ¸©åº¦é€‰æ‹©
            show_retrive=st.checkbox("å±•ç¤ºæ£€ç´¢æ¥æº",value=False)
            st.button(label="æ¸…é™¤èŠå¤©è®°å½•", on_click=lambda: clear(),use_container_width=True) #æ¸…é™¤èŠå¤©è®°å½•æŒ‰é’®
    st.title("ğŸ’° é‡‘èèµ„è®¯æœºå™¨äºº")
    st.subheader(body='',divider="rainbow")

    '''æ»šåŠ¨æ›´æ–°èŠå¤©è®°å½•'''
    with st.chat_message("AI"):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯AIé‡‘èèµ„è®¯æœºå™¨äººï¼Œæˆ‘ä¼šæ ¹æ®æœ€æ–°çš„ä¸€äº›é‡‘èæ–°é—»å›ç­”ä½ çš„é—®é¢˜ã€‚æ­¤å¤–åœ¨æˆ‘çš„å·¦ä¾§æ ä¸­ï¼Œæ‚¨å¯ä»¥æ›´æ¢ä¸åŒçš„AIæ¨¡å‹ã€‚")
    for message in st.session_state.finews_message:
        if isinstance(message, AIMessage):
            with st.chat_message("AI"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("Human"):
                st.markdown(message.content)
    finews_vectorstore_flag=st.session_state.finews_vectorstore
    if finews_vectorstore_flag is None:
        with st.chat_message("AI"):
            with st.status("æ­£åœ¨è·å–æœ€æ–°çš„é‡‘èèµ„è®¯", expanded=True) as status:
                st.markdown("è·å–æœ€æ–°æ•°æ®åº“...")
                root_dir,latest_db_name=get_latest_db_file() #è·å–æœ€æ–°çš„æ•°æ®åº“æ–‡ä»¶
                if latest_db_name is None:
                    st.markdown("æ•°æ®åº“è·å–å¤±è´¥")
                    status.update(label="è·å–å¤±è´¥ï¼Œè¯·ç¨åå°è¯•æˆ–è€…åœ¨Abouté¡µé¢æäº¤Issue", state="error", expanded=False)
                else:
                    collection_name="vectorstore_"+latest_db_name.split("_")[-1]
                    db_name=os.path.join(root_dir,latest_db_name)
                    print(db_name)
                    st.session_state.finews_vectorstore = Chroma(collection_name=collection_name,persist_directory=db_name, embedding_function=CohereEmbeddings(model="embed-multilingual-v3.0"))
                    if st.session_state.finews_vectorstore is not None:
                        st.markdown("æ•°æ®åº“è·å–æˆåŠŸ")
                        status.update(label="æ•°æ®åº“è·å–æˆåŠŸ", state="complete", expanded=False)
                    else:
                        st.markdown("æ•°æ®åº“è·å–å¤±è´¥")
                        status.update(label="æ•°æ®åº“è·å–å¤±è´¥", state="error", expanded=False)
        finews_vectorstore_flag=st.session_state.finews_vectorstore

    if finews_vectorstore_flag is not None:
        '''ç”¨æˆ·é—®é¢˜äº¤äº’'''
        question = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜")
        if question:
            with st.chat_message("Human"):
                st.markdown(question)
                st.session_state.finews_message.append(HumanMessage(content=question))#æ·»åŠ ç”¨æˆ·é—®é¢˜èŠå¤©è®°å½•
            with st.chat_message("AI"):
                with st.spinner('æ£€ç´¢ä¸­....'):
                    chat_history = st.session_state.finews_message
                    vectorstore = st.session_state.finews_vectorstore
                    # response=st.session_state.finews_bot.get_response(question,chat_history,vectorstore)
                    st.session_state.finews_bot.init_llm_model(select_platform,select_model,select_temperature)
                    response = st.write_stream(st.session_state.finews_bot.get_response(question,chat_history,vectorstore))#æµå¼è¾“å‡º
                    st.session_state.finews_message.append(AIMessage(content=response))
                if show_retrive:#å±•ç¤ºæ£€ç´¢æ¥æº
                    with st.expander("æ¥æºå†…å®¹"):
                        for i in range(len(st.session_state.finews_bot.context)):
                            st.markdown(f"ç¬¬{i}ä¸ªç›¸å…³å†…å®¹: {st.session_state.finews_bot.context[i].page_content}")
