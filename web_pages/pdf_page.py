import random
import time
from langchain_openai import ChatOpenAI
import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langchain_community.chat_models import QianfanChatEndpoint
from config_setting import model_config
from langchain_community.document_loaders import PDFMinerLoader
from dotenv import find_dotenv, load_dotenv
import tempfile
import os
from config_setting import prompt_config
class pdfbot:
    def __init__(self):
        """
        åˆå§‹åŒ–PDFBotç±»çš„å®ä¾‹ï¼ŒåŠ è½½ç¯å¢ƒå˜é‡å¹¶è®¾ç½®æ¨¡å‹é€‰é¡¹å’Œä»¤ç‰Œæ•°ã€‚
        """
        load_dotenv(find_dotenv())
        self.llm = None

    def judge_type_prompt(self,pdf_type):
        """
        æ ¹æ®PDFç±»å‹é€‰æ‹©ç›¸åº”çš„æç¤ºæ¨¡æ¿ã€‚

        Parameters:
        pdf_type (str): PDFçš„ç±»å‹ã€‚

        Returns:
        str: é€‰æ‹©çš„æç¤ºæ¨¡æ¿ã€‚
        """
        if pdf_type=='ç®€å†åˆ†æ':
            prompt=prompt_config.resume_prompt
        # elif pdf_type=='å­¦æœ¯è®ºæ–‡è§£æ':
        #     prompt=prompt_config.academic_prompt
        # else:
        #     prompt=prompt_config.general_prompt
        return prompt

    def init_llm_model(self,select_platform,select_model,select_temperature):
        """
        åˆå§‹åŒ–æ¨¡å‹ã€‚

        Parameters:
        select_platform (str): é€‰æ‹©çš„æ¨¡å‹å¹³å°ã€‚
        select_model (str): é€‰æ‹©çš„æ¨¡å‹ã€‚
        select_temperature (float): æ¸©åº¦ç³»æ•°ã€‚
        """
        self.model_option = select_model
        self.model_tokens = model_config.model_description_ls[select_model]["tokens"]
        if select_platform=='ç™¾åº¦äº‘å¹³å°':
            self.llm = QianfanChatEndpoint(model=select_model,temperature=select_temperature)
        elif select_platform=='Groqå¹³å°':
            self.llm = ChatGroq(model_name=select_model,max_tokens=self.model_tokens,temperature=select_temperature)
        elif select_platform=='Siliconflowå¹³å°':
            self.llm = ChatOpenAI(model_name=select_model,base_url="https://api.siliconflow.cn/v1",temperature=select_temperature)
    
    def get_response(self,question,pdf_type,pdf_content,chat_history):
        """
        æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ã€PDFç±»å‹ã€PDFå†…å®¹å’Œå¯¹è¯å†å²è·å–å“åº”ã€‚

        Parameters:
        question (str): ç”¨æˆ·çš„é—®é¢˜ã€‚
        pdf_type (str): PDFçš„ç±»å‹ã€‚
        pdf_content (str): PDFçš„å†…å®¹ã€‚
        chat_history (list): å¯¹è¯å†å²åˆ—è¡¨ã€‚

        Returns:
        stræˆ–generator: å¦‚æœä½¿ç”¨æµå¼è¾“å‡ºï¼Œè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼›å¦åˆ™è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
        """
        try:
            prompt_selected=self.judge_type_prompt(pdf_type)
            prompt = ChatPromptTemplate.from_template(prompt_selected)
            chain = prompt | self.llm | StrOutputParser()
            return chain.stream({
                "chat_history": chat_history,
                "question": question,
                "resume_content": pdf_content
            })
        except Exception as e:
            return f"å½“å‰æ¨¡å‹{self.model_option}æš‚ä¸å¯ç”¨ï¼Œè¯·åœ¨å·¦ä¾§æ é€‰æ‹©å…¶ä»–æ¨¡å‹ã€‚"

def init_params():
    """
    åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å‚æ•°ã€‚

    å¦‚æœä¼šè¯çŠ¶æ€ä¸­ä¸å­˜åœ¨"pdf_message"é”®ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨å¹¶å°†å…¶èµ‹å€¼ç»™"pdf_message"ã€‚
    å¦‚æœä¼šè¯çŠ¶æ€ä¸­ä¸å­˜åœ¨"pdf"é”®ï¼Œåˆ™å°†å…¶åˆå§‹åŒ–ä¸ºNoneã€‚
    å¦‚æœä¼šè¯çŠ¶æ€ä¸­ä¸å­˜åœ¨"pdf_bot"é”®ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„PDFBotå®ä¾‹å¹¶å°†å…¶èµ‹å€¼ç»™"pdf_bot"ã€‚
    å¦‚æœä¼šè¯çŠ¶æ€ä¸­ä¸å­˜åœ¨"pdf_content"é”®ï¼Œåˆ™å°†å…¶åˆå§‹åŒ–ä¸ºNoneã€‚
    å¦‚æœä¼šè¯çŠ¶æ€ä¸­ä¸å­˜åœ¨"resume_summary"é”®ï¼Œåˆ™å°†å…¶åˆå§‹åŒ–ä¸ºNoneã€‚
    """
    if "pdf_message" not in st.session_state:
        st.session_state.pdf_message = []
    if "pdf" not in st.session_state:
        st.session_state.pdf= None
    if "pdf_bot" not in st.session_state:
        st.session_state.pdf_bot = pdfbot()
    if "pdf_content" not in st.session_state:
        st.session_state.pdf_content = None
    if "resume_summary" not in st.session_state:
        st.session_state.resume_summary = None


def clear():
    """
    æ¸…é™¤ä¼šè¯çŠ¶æ€ä¸­çš„PDFç›¸å…³è®°å½•å’ŒPDFBotå®ä¾‹ã€‚

    å°†ä¼šè¯çŠ¶æ€ä¸­çš„"pdf_message"é”®å¯¹åº”çš„å€¼é‡ç½®ä¸ºç©ºåˆ—è¡¨ã€‚
    åˆ›å»ºä¸€ä¸ªæ–°çš„PDFBotå®ä¾‹å¹¶å°†å…¶èµ‹å€¼ç»™"pdf_bot"é”®ã€‚
    å°†"pdf"ã€"pdf_content"å’Œ"resume_summary"é”®å¯¹åº”çš„å€¼é‡ç½®ä¸ºNoneã€‚
    """
    st.session_state.pdf_message = []
    st.session_state.pdf_bot = pdfbot()
    st.session_state.pdf= None
    st.session_state.pdf_content = None
    st.session_state.resume_summary=None
    

def summary_resume(pdf_content,select_platform,select_model):
    """
    ç”ŸæˆPDFç®€å†çš„æ‘˜è¦ã€‚

    Parameters:
    pdf_content (str): PDFç®€å†çš„å†…å®¹ã€‚
    model_option (str): é€‰æ‹©çš„æ¨¡å‹ã€‚
    model_tokes (int): æ¨¡å‹çš„æœ€å¤§ä»¤ç‰Œæ•°ã€‚

    Returns:
    stræˆ–generator: å¦‚æœä½¿ç”¨æµå¼è¾“å‡ºï¼Œè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼›å¦åˆ™è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
    """
    try:
        select_temperature=0.2
        model_tokens = model_config.model_description_ls[select_model]["tokens"]
        if select_platform=='ç™¾åº¦äº‘å¹³å°':
            llm = QianfanChatEndpoint(model=select_model,temperature=select_temperature)
        elif select_platform=='Groqå¹³å°':
            llm = ChatGroq(model_name=select_model,max_tokens=model_tokens,temperature=select_temperature)
        elif select_platform=='Siliconflowå¹³å°':
            llm = ChatOpenAI(model_name=select_model,base_url="https://api.siliconflow.cn/v1",temperature=select_temperature)
        prompt_selected=prompt_config.resume_summary_prompt
        prompt = ChatPromptTemplate.from_template(prompt_selected)
        chain = prompt | llm | StrOutputParser()
        return chain.stream({
            "resume_content": pdf_content,
        })
    except Exception as e:
        return f"å½“å‰æ¨¡å‹{select_model}æš‚ä¸å¯ç”¨ï¼Œè¯·åœ¨å·¦ä¾§æ é€‰æ‹©å…¶ä»–æ¨¡å‹ã€‚"
    

def pdf_page():
    init_params()
    '''é¡µé¢å¸ƒå±€'''
    with st.sidebar:
        with st.container(border=True):
            select_platform=st.selectbox("é€‰æ‹©æ¨¡å‹å¹³å°",options=list(model_config.model_platform_ls.keys()))#æ¨¡å‹é€‰æ‹©
            select_model=st.selectbox("é€‰æ‹©æ¨¡å‹",options=model_config.model_platform_ls[select_platform]) 
            select_temperature=st.slider("æ¸©åº¦ç³»æ•°",min_value=0.1,max_value=1.0,step=0.1,value=0.7,help='æ•°å€¼ä½è¾“å‡ºæ›´å…·ç¡®å®šå’Œä¸€è‡´æ€§ï¼Œæ•°å€¼é«˜æ›´å…·åˆ›é€ å’Œå¤šæ ·æ€§')#æ¸©åº¦é€‰æ‹©
            st.button(label="æ¸…é™¤èŠå¤©è®°å½•", on_click=lambda: clear(),use_container_width=True)
    st.title("ğŸ—pdfè§£æ-AIæœºå™¨äºº")
    st.subheader(body='',divider="rainbow")

    '''åˆå§‹åŒ–æ¶ˆæ¯'''
    #åˆå§‹åŒ–æ¶ˆæ¯
    with st.chat_message("AI"):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯åŸºäºPDFè§£æçš„AIèŠå¤©æœºå™¨äºº,è¯·ä½ å…ˆä¸Šä¼ ä½ çš„PDFæ–‡ä»¶ï¼Œç„¶åè¾“å…¥ä½ çš„é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›å›ç­”ä½ çš„é—®é¢˜")
    

    '''PDFä¸Šä¼ '''
    with st.container(border=True):
        st.session_state.pdf = st.file_uploader('è¯·ä¸Šä¼ ä½ çš„PDF', type="pdf")
        pdf_type=st.selectbox("é€‰æ‹©PDFè§£æåŠŸèƒ½",options=['ç®€å†åˆ†æ'],index=0)
        upload_btn= st.button('ä¸Šä¼ æ–‡æ¡£')



    if upload_btn:#ä¸Šä¼ PDFæ–‡ä»¶
        st.session_state.pdf_message=[] #åˆå§‹åŒ–pdfå†…å®¹
        if st.session_state.pdf is not None:
            '''PDFè§£æ'''
            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                tmp_file.write(st.session_state.pdf.read())
            loader = PDFMinerLoader(tmp_file.name)
            st.session_state.pdf_content = loader.load_and_split()
            os.remove(tmp_file.name)
            '''å…ˆæ•´ä½“å›ç­”PDFé—®é¢˜'''
            if pdf_type=='ç®€å†åˆ†æ':
                with st.chat_message("AI"):
                    st.session_state.resume_summary = st.write_stream(summary_resume(st.session_state.pdf_content,select_platform,select_model))#ç®€å†åˆ†æ
                    st.session_state.pdf_message.append(AIMessage(content=st.session_state.resume_summary))
        else:
            st.warning("è¯·å…ˆä¸Šä¼ Pæ­£ç¡®çš„PDFæ–‡ä»¶")

    '''æ»šåŠ¨æ›´æ–°èŠå¤©è®°å½•'''
    for message in st.session_state.pdf_message:
        if isinstance(message, AIMessage):
            with st.chat_message("AI"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("Human"):
                st.markdown(message.content)

    '''ç”¨æˆ·é—®é¢˜äº¤äº’'''
    question = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜")
    if question and st.session_state.pdf_content is not None:
        with st.chat_message("Human"):
            st.markdown(question)
            st.session_state.pdf_message.append(HumanMessage(content=question))
        with st.chat_message("AI"):
            st.session_state.pdf_bot.init_llm_model(select_platform,select_model,select_temperature)
            response = st.write_stream(st.session_state.pdf_bot.get_response(question,pdf_type,st.session_state.pdf_content,st.session_state.pdf_message))
            st.session_state.pdf_message.append(AIMessage(content=response))


