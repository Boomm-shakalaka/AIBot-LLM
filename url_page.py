import time
import streamlit as st
from config_setting import model_config, func_modules,prompt_config
from st_pages import Page, Section, show_pages, add_page_title
import requests
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.messages import AIMessage, HumanMessage


########################################
#æœºå™¨äººæ¨¡å‹
########################################
from dotenv import find_dotenv, load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_history_aware_retriever
from langchain_groq import ChatGroq
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.chains import create_history_aware_retriever
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough,RunnableParallel
from langchain_cohere import CohereEmbeddings
import os

# os.environ["LANGCHAIN_TRACING_V2"] = "true"  #LangSmith init
load_dotenv(find_dotenv())
def retrieve_data(docs):
    try:
        text_splitter = RecursiveCharacterTextSplitter()
        splits = text_splitter.split_documents(docs)
        # bge_embeddings = HuggingFaceBgeEmbeddings(model_name="moka-ai/m3e-base")
        embeddings = CohereEmbeddings(model="embed-multilingual-light-v3.0")
        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
        return vectorstore.as_retriever()
    except Exception as e:
        return f"å½“å‰æ–‡æœ¬åµŒå…¥å‘é‡å¤„ç†äº§ç”Ÿé—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ï¼Œæˆ–è€…é€‰æ‹©å…¶ä»–AIåŠŸèƒ½"

def get_response_langsmith(model_option,retriever,question):
    llm = ChatGroq(model_name=model_option)
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    rag_chain_from_docs = (
        RunnablePassthrough.assign(context=(lambda x: format_docs(x["context"])))
        | prompt_config.rag_prompt
        | llm
        | StrOutputParser()
    )

    # rag_chain_with_source = RunnableParallel(
    #     {"context": retriever, "question": RunnablePassthrough()}
    # ).assign(answer=rag_chain_from_docs)
    rag_chain_with_source=RunnableParallel(
        {"context": retriever, "question": RunnablePassthrough()}
    ).assign(answer=rag_chain_from_docs)
    for chunk in rag_chain_with_source.stream(question):
        print(chunk)
    return ''


def get_response(model_option,retriever,question,chat_history):
    try:
        llm = ChatGroq(model_name=model_option)
        # Create the chain for historical messages
        contextualize_q_prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", prompt_config.contextualize_q_system_prompt_en),
                    MessagesPlaceholder("chat_history"),
                    ("human", "{input}"),
                ]
            )
        history_aware_retriever = create_history_aware_retriever(
            llm, retriever, contextualize_q_prompt
        )
        # Create the chain for full question and answer
        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", prompt_config.qa_system_prompt_en),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
        result=rag_chain.invoke({
            "input": question, 
            "chat_history": chat_history
            })
        return result['answer']
    except Exception as e:
        return f"å½“å‰æ£€ç´¢æš‚ä¸å¯ç”¨ï¼Œè¯·åœ¨å·¦ä¾§æ æ›´æ¢æ¨¡å‹ï¼Œæˆ–è€…é€‰æ‹©å…¶ä»–åŠŸèƒ½ã€‚"



########################################
#åˆå§‹åŒ–å‚æ•°
########################################
#åˆå§‹åŒ–æ¶ˆæ¯"
init_url_message = "æ‚¨å¥½ï¼Œæˆ‘æ˜¯åŸºäºAIå¤§æ¨¡å‹çš„ç½‘é¡µè§£ææœºå™¨äººã€‚æ‚¨å¯ä»¥ç°åœ¨ä¸‹æ–¹èŠå¤©æ¡†è¾“å…¥æƒ³è¦æœç´¢çš„URLï¼Œæˆ‘ä¼šæ ¹æ®è¯¥ç½‘é¡µå†…å®¹å°½\
                    å¯èƒ½åœ°å›ç­”æ‚¨çš„é—®é¢˜ã€‚"

#åˆå§‹åŒ–é¡µé¢èŠå¤©è®°å½•
if "url_history" not in st.session_state:
    st.session_state.url_history = [AIMessage(content=init_url_message)]

#åˆå§‹æ¨¡å‹èŠå¤©è®°å½•
if "url_chat_history" not in st.session_state:
    st.session_state.url_chat_history = [
        AIMessage(content=init_url_message),
    ]
#åˆå§‹åŒ–URLé“¾æ¥
if "url" not in st.session_state:
    st.session_state.url=''
#åˆå§‹åŒ–å‘é‡
if "retriever" not in st.session_state:
    st.session_state.retriever = None
#åˆå§‹åŒ–æ¨¡å‹å›ç­”
if "response" not in st.session_state:
    st.session_state.response=''

########################################
#å‰ç«¯é¡µé¢è®¾è®¡ä¸åŠŸèƒ½å¼€å‘
########################################
#é¡µé¢title
set_page_config=st.set_page_config(
        page_title="AIBot", 
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="auto",  #ä¾§è¾¹æ æ˜¯å¦å›ºå®š,"auto", "expanded", or "collapsed"
        # menu_items={
        #     'Get Help': 'https://www.extremelycoolapp.com/help',
        #     'Report a bug': "https://www.extremelycoolapp.com/bug",
        #     'About': "# This is a header. This is an *extremely* cool app!"
        # }

)


#é¡µé¢å¯¼èˆª
add_page_title() #æœ‰è¿™ä¸ªshow_pagesæ‰èƒ½ç”Ÿæ•ˆ
show_pages(
    [    
        # Section(name='AIåŠŸèƒ½',icon='ğŸ¤–'),
        Page("chat_page.py", "AI-èŠå¤©æœºå™¨äºº"),
        Page("url_page.py", "URLæ£€ç´¢æœºå™¨äºº"),
        Page("pdf_page.py", "AI-PDFè§£ææœºå™¨äºº"),
        # Page("summary_page.py", "æ‘˜è¦-AIåˆ†ææœºå™¨äºº"),
        # Page("AboutPage.py", "å…³äºAbout")
    ]
)
#å·¦ä¾§æ 
with st.sidebar:
    #æ¨¡å‹é€‰æ‹©
    model_option = st.selectbox(
        "é€‰æ‹©æœºå™¨äººæ¨¡å‹:",
        options=list(model_config.models.keys()),
        format_func=lambda x: model_config.models[x]["name"],
        index=4  # Default to mixtral
    )
    #æ¸…é™¤èŠå¤©è®°å½•
    def clear_chat_history():
        st.session_state.url_history = [AIMessage(content=init_url_message),]#åˆå§‹åŒ–æ¨¡å‹èŠå¤©è®°å½•
        st.session_state.url_chat_history = [AIMessage(content=init_url_message),]#åˆå§‹æ¨¡å‹èŠå¤©è®°å½•
        st.session_state.url=''#åˆå§‹åŒ–URLé“¾æ¥
        st.session_state.retriever = None#åˆå§‹åŒ–å‘é‡
        st.session_state.response=''#åˆå§‹åŒ–æ¨¡å‹å›ç­”
    
    st.button("æ¸…é™¤è®°å½•", on_click=lambda: clear_chat_history())



#ç½‘é¡µèŠå¤©è®°å½•
for message in st.session_state.url_history:
    if isinstance(message, AIMessage):
        with st.chat_message("AI"):
            st.write(message.content)
    elif isinstance(message, HumanMessage):
        with st.chat_message("Human"):
            st.write(message.content)

def parse_data(url):
    loader = WebBaseLoader(url)
    docs = loader.load()
    return docs


def request_website(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # If the response status code is not 200, an exception is thrown
        return response.text
    except requests.RequestException as e:
        return None
    

#è¾“å…¥é—®é¢˜
question = st.chat_input('example: https://www.baidu.com')
if question:
    if st.session_state.url=='': #æ²¡æœ‰ç»™ç½‘ç«™é“¾æ¥
        url_response=request_website(question)
        if url_response:  #æ˜¯URL
            with st.chat_message("AI"):
                with st.status("Load Website, please wait..", expanded=True) as status:
                    st.write("Parsing data on URL...")
                    docs=parse_data(question)
                    st.write("Embedding the data...")
                    st.session_state.retriever=retrieve_data(docs)
                    status.update(label="Parsing complete!", state="complete", expanded=False)
                tmp_text=f"\n\nå½“å‰è®¿é—®URLä¸º: {question}.\n\næ‚¨å¯ä»¥è¯¢é—®å…³äºè¯¥ç½‘é¡µçš„é—®é¢˜ã€‚\
                                å¦‚æœæ‚¨æƒ³æŸ¥è¯¢åˆ«çš„ç½‘é¡µ,è¯·ç›´æ¥è¾“å…¥æ–°çš„URL "
                st.markdown(tmp_text)
                st.session_state.url=question  #ä¿å­˜URL
                st.session_state.url_history = [] #ç½‘é¡µèŠå¤©è®°å½•åˆå§‹åŒ–
                st.session_state.url_history.append(AIMessage(content=tmp_text))
                st.session_state.url_chat_history = [AIMessage(content=init_url_message),] #æ¨¡å‹èŠå¤©è®°å½•åˆå§‹åŒ–
        else: #æ— æ³•è®¿é—®
            tmp_text=f'URL: ( { question } ) æ— æ³•è®¿é—®,è¯·æ£€æŸ¥ç½‘é¡µæ˜¯å¦æ­£ç¡®æˆ–è€…è¯¥ç½‘é¡µæ— æ³•è¿æ¥(éœ€è¦åŒ…å«httpæˆ–è€…https)'
            with st.chat_message("AI"):
                st.markdown(tmp_text)
            st.session_state.url_history.append(AIMessage(content=tmp_text))
    else:#å·²ç»æœ‰urlï¼Œè¦ä¹ˆæ˜¯æç¤ºè¯ï¼Œè¦ä¹ˆæ˜¯æ–°çš„url
        url_response=request_website(question)
        if url_response:#æ–°çš„url
            with st.chat_message("AI"):
                with st.status("Load Website, please wait..", expanded=True) as status:
                    st.write("Parsing data on URL...")
                    docs=parse_data(question)
                    st.write("Embedding the data...")
                    st.session_state.retriever=retrieve_data(docs)
                    status.update(label="Parsing complete!", state="complete", expanded=False)
                tmp_text=f"\n\nå½“å‰è®¿é—®URLä¸º: {question}.\n\næ‚¨å¯ä»¥è¯¢é—®å…³äºè¯¥ç½‘é¡µçš„é—®é¢˜ã€‚\
                                å¦‚æœæ‚¨æƒ³æŸ¥è¯¢åˆ«çš„ç½‘é¡µ,è¯·ç›´æ¥è¾“å…¥æ–°çš„URL "
                st.markdown(tmp_text)
                st.session_state.url=question  #ä¿å­˜URL
                st.session_state.url_history = [] #ç½‘é¡µèŠå¤©è®°å½•åˆå§‹åŒ–
                st.session_state.url_history.append(AIMessage(content=tmp_text))
                st.session_state.url_chat_history = [AIMessage(content=init_url_message),] #æ¨¡å‹èŠå¤©è®°å½•åˆå§‹åŒ–
        else:#æç¤ºè¯
            with st.chat_message("Human"):
                st.markdown(question)
            st.session_state.url_history.append(HumanMessage(content=question)) #æ·»åŠ ç½‘é¡µè®°å½•
            st.session_state.url_chat_history.append(HumanMessage(content=question))  #æ·»åŠ æ¨¡å‹é—®é¢˜è®°å½•
            with st.chat_message("AI"):
                with st.spinner('æ£€ç´¢ä¸­....'):
                    # response=st.write_stream(get_response_langsmith(model_option,st.session_state.retriever,question))
                    st.session_state.response = get_response(model_option,st.session_state.retriever,question, st.session_state.url_chat_history)
                st.markdown(st.session_state.response)          
            st.session_state.url_history.append(AIMessage(content=st.session_state.response))#æ·»åŠ ç½‘é¡µè®°å½•
            st.session_state.url_chat_history.append(AIMessage(content=st.session_state.response))  #æ·»åŠ æ¨¡å‹é—®é¢˜è®°å½•



                

